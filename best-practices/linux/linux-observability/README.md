# Linux Observability Tools â€” A Practical Guide

Observability is the ability to understand what a Linux system is doing *internally* by examining the signals it emits â€” metrics, logs, traces, and events.  
This guide provides a structured overview of Linux observability tools, grouped by the system layers they inspect. It is designed as a practical reference for troubleshooting, performance engineering, capacity planning, and DevSecOps workflows.

---

## Table of Contents
- [ğŸ§± 1. Application & User-Space Observability](#-1-application--user-space-observability)
- [ğŸ§© 2. System Libraries & Syscall Interface](#-2-system-libraries--syscall-interface)
- [ğŸ§¬ 3. Kernel Subsystems Observability](#-3-kernel-subsystems-observability)
- [ğŸ”© 4. Device Drivers & Block Layer Observability](#-4-device-drivers--block-layer-observability)
- [ğŸ“¦ 5. Storage & Swap Observability](#-5-storage--swap-observability)
- [ğŸŒ 6. Network Stack & NIC Observability](#-6-network-stack--nic-observability)
- [ğŸ–¥ï¸ 7. Hardware Observability (CPU, RAM, Buses)](#-7-hardware-observability-cpu-ram-buses)
- [ğŸ“Š 8. System-Wide Observability Tools](#-8-system-wide-observability-tools)

---

## ğŸ“Š Overview Diagram

![Linux Observability Tools](linux-observability-diagram.png)

> Diagram Â© Brendan Gregg â€” used here with attribution for educational and informational purposes.  

The diagram maps common observability tools to layers of the Linux operating system, from user-space applications down to hardware, providing a mental model for selecting the right tool during analysis or incident response.

---

# ğŸ§± 1. Application & User-Space Observability

These tools inspect behaviour at the *process and application* level, including interactions with system libraries.

### ğŸ”§ Tools
- **strace** â€“ traces system calls made by an application.  
- **ltrace** â€“ traces dynamic library calls.  
- **ss** â€“ modern socket statistics (replacement for `netstat`).  
- **netstat** â€“ legacy but still useful for connection state overview.  
- **sysdig** â€“ system-wide syscall/event capture and filtering.  
- **lsof** â€“ lists open files, sockets, pipes, etc.  
- **pidstat** â€“ per-process CPU, memory, I/O, threads.  
- **pcstat** â€“ page cache statistics for specific files.

### ğŸ§  When to use
- Debugging why an application is slow or blocked.  
- Identifying network usage per process.  
- Auditing open files and ports.  
- Understanding syscall patterns for performance tuning.

---

# ğŸ§© 2. System Libraries & Syscall Interface

This layer sits between applications and the kernel. Tools here help examine transitions between user-space and kernel-space.

### ğŸ”§ Tools
- **strace / ltrace** â€“ observe execution flow into syscalls and libraries.  
- **perf** â€“ syscall latency, profiling, hotspots.  
- **ftrace** â€“ built-in kernel tracer for syscalls and function calls.  
- **SystemTap (stap)** â€“ programmable probes for syscalls.  
- **LTTng** â€“ high-performance tracing for production systems.  
- **eBPF / bpftrace** â€“ modern, safe kernel-level instrumentation.

### ğŸ§  When to use
- Diagnosing syscall bottlenecks.  
- Monitoring unexpected kernel interactions.  
- High-resolution production tracing with low overhead (eBPF).

---

# ğŸ§¬ 3. Kernel Subsystems Observability

The kernel handles filesystems, memory management, scheduling, and networking. Tools here inspect these internal mechanisms.

### ğŸ”§ Tools
- **perf** â€“ scheduler behaviour, CPU cycles, kernel hotspots.  
- **tcpdump** â€“ raw packet capture at the IP/Ethernet layers.  
- **iptraf** â€“ lightweight network utilisation monitor.  
- **vmstat** â€“ processes, memory, swap, I/O, interrupts.  
- **slabtop** â€“ kernel slab allocator usage.  
- **free** â€“ memory allocation breakdown.  
- **pidstat** â€“ scheduler awareness and per-thread stats.  
- **tiptop** â€“ per-thread metrics using hardware counters.

### ğŸ§  When to use
- Identifying memory pressure, leaks, or slab exhaustion.  
- Determining network packet loss or congestion.  
- Analysing scheduler-induced latency.  
- Understanding kernel-side performance issues.

---

# ğŸ”© 4. Device Drivers & Block Layer Observability

These tools examine I/O as it flows through the Linux block subsystem.

### ğŸ”§ Tools
- **iostat** â€“ block device throughput and latency.  
- **iotop** â€“ per-process disk I/O usage.  
- **blktrace** â€“ very detailed block layer tracing.  
- **perf / tiptop** â€“ device driver profiling.

### ğŸ§  When to use
- Troubleshooting slow disk I/O.  
- Detecting I/O starvation or noisy-neighbour workloads.  
- Analysing LVM/RAID performance issues.

---

# ğŸ“¦ 5. Storage & Swap Observability

Tools focusing on physical disks, logical volumes, controllers, and swap usage.

### ğŸ”§ Tools
- **iostat** â€“ read/write performance.  
- **iotop** â€“ which processes are causing I/O.  
- **blktrace** â€“ kernel-level I/O event tracing.  
- **swapon -s** â€“ view swap devices and utilisation.

### ğŸ§  When to use
- Swap thrash detection.  
- Disk queue depth analysis.  
- Understanding storage behaviour under load.

---

# ğŸŒ 6. Network Stack & NIC Observability

These tools examine network interfaces, Ethernet drivers, ports, and NIC statistics.

### ğŸ”§ Tools
- **tcpdump** â€“ packet-level visibility.  
- **ss / netstat** â€“ connections and sockets.  
- **iptraf** â€“ per-interface traffic charts.  
- **ethtool** â€“ NIC driver settings and link state.  
- **nicstat** â€“ interface utilisation.  
- **lldptool** â€“ LLDP neighbour discovery.  
- **snmpget** â€“ SNMP-based network metrics.

### ğŸ§  When to use
- Packet drops, retransmits, or MTU mismatches.  
- NIC offload tuning (TSO, GRO, etc.).  
- Link speed/duplex mismatch troubleshooting.

---

# ğŸ–¥ï¸ 7. Hardware Observability (CPU, RAM, Buses)

These tools provide insights into how the **hardware itself** behaves â€” including CPU frequency, power states, performance counters, NUMA locality, memory pressure, cache behaviour, and bus throughput.

## ğŸ”§ CPU Tools

- **mpstat** â€“ Reports CPU usage per core, showing utilisation, steal time, IRQ time, and more.
- **top** â€“ Real-time process monitoring with CPU, load average, and per-thread breakdowns.
- **ps** â€“ Snapshot of process states, CPU usage, memory usage, and scheduling information.
- **pidstat** â€“ Per-thread and per-process CPU utilisation, context switching, and scheduling metrics.
- **perf** â€“ Hardware performance counter profiler (cycles, cache misses, branch mispredictions).
- **turbostat** â€“ Intel-specific tool showing CPU frequencies, C-states, P-states, and turbo boost behaviour.
- **rdmsr** â€“ Reads CPU model-specific registers (MSRs) for extremely low-level introspection.

---

## ğŸ”§ Memory Tools

- **vmstat** â€“ Shows paging, swapping, memory pressure, interrupts, and system-wide throughput.
- **free** â€“ Reports total, used, cached, and available system memory.
- **slabtop** â€“ Displays kernel slab allocator statistics (caches, objects, memory used).
- **numastat** â€“ NUMA locality, node memory distribution, and remote memory access counts.
- **perf (memory events)** â€“ Analyses hardware counters related to RAM, cache, and memory bus traffic.

---

## ğŸ§  When to use
- NUMA locality and cross-node memory access debugging.  
- CPU throttling, frequency scaling, or thermal throttling investigations.  
- Memory pressure analysis, leaking workloads, or kernel slab issues.  
- High-performance tuning for compute-heavy or latency-sensitive workloads.

---

# ğŸ“Š 8. System-Wide Observability Tools

These tools cover multiple layers at once.

### ğŸ”§ Tools
- **sar** â€“ historic performance logs across CPU, memory, I/O, network.  
- **dstat** â€“ live multi-metric system aggregation.  
- **sysdig** â€“ holistic tracing across syscalls, network, containers.  
- **/proc** â€“ raw kernel data for metrics, states, drivers, and interfaces.

### ğŸ§  When to use
- Incident response and baselining.  
- Long-term trending and anomaly detection.  
- System-wide correlation across resources.

---

# ğŸ› ï¸ Practical Use Cases

### âœ” Root Cause Analysis (RCA)
- Identify if a slowdown is CPU, memory, network, or storage related.  
- Trace a misbehaving process through syscalls into the kernel.  
- Compare observed performance against baseline.

### âœ” Performance Tuning
- Scheduler tracing for latency-sensitive workloads.  
- NIC tuning via `ethtool` for high-throughput environments.  
- Storage insight for LVM/RAID/SSD/HDD tuning.

### âœ” DevSecOps / Security
- eBPF tools for detecting suspicious syscalls.  
- `lsof` for auditing unexpected open sockets/files.  
- `sysdig` rules for behavioural anomaly detection.

A secure system is one that is *understood*, not just hardened.

---

# ğŸ” Observability in DevSecOps

Observability is not just operational â€” it is *security-critical*:

- Detect unusual syscall patterns (possible intrusion).  
- Identify crypto miners via CPU and scheduler patterns.  
- Spot exfiltration via abnormal NIC or TCP behaviour.  
- Validate hardening changes improve performance rather than degrade it.

---

# ğŸ§­ Future Enhancements

Potential expansions to this guide:

- eBPF cookbook examples (uprobe, kprobe, tracepoint).  
- Container observability tools (cAdvisor, bcc tools).  
- Systemd journal analysis patterns.  
- Kernel flamegraphs and performance visualisation.

PRs and contributions are welcome.

---

# ğŸ“š References

- Brendan Gregg â€” *Linux Performance Tools*  
- Kernel documentation â€” https://www.kernel.org/doc/  
- Sysdig, LTTng, SystemTap official docs  
- eBPF / bpftrace reference guides  
