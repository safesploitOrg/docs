# üß† Ceph Public Network Migration (Proxmox)

**172.16.0.0/16 ‚Üí 10.50.0.0/24**  
_No service downtime, no data loss_

## üìå Context

This procedure documents a live Ceph public network migration performed on a Proxmox-backed Ceph cluster.  
The goal was to eliminate management-network congestion while maintaining cluster availability and data integrity.

---

  - [üéØ Objective](#-objective)
  - [üß± Key Concepts (Read Once)](#-key-concepts-read-once)
  - [üö® Troubleshooting](#-troubleshooting-osds-not-reachable--wrong-subnet)
  - [‚ö†Ô∏è Risks Considered](#-risks-considered)
  - [‚úÖ Final State](#-final-state)

---

## üéØ Objective

Migrate **all Ceph traffic** (MON, MGR, MDS, OSD front + back) from a congested management network to a **dedicated Ceph fabric** (e.g. 2.5 GbE switch), while keeping the cluster healthy and online.

--- 

## üß± Key Concepts (Read Once)

### `public_network`
- Client ‚Üî OSD traffic  
- MON / MGR control plane  
- CephFS metadata traffic

### `cluster_network`
- OSD ‚Üî OSD replication & recovery (data plane)

### Important behaviours
- **MON & MGR enforce address validation**
- **OSDs bind addresses at restart**
- `/etc/pve/ceph.conf` is **not authoritative alone** ‚Äî Ceph also uses its **internal config database**

---

## 1Ô∏è‚É£ Prepare the New Ceph Network

Create a **dedicated bridge** on each node (example: `vmbr-ceph`):

```bash
vim /etc/network/interfaces
```

```ini
auto vmbr-ceph
iface vmbr-ceph inet static
    address 10.50.0.20/24
    bridge-ports eno2
    bridge-stp off
    bridge-fd 0
# Ceph (Fabric)
```

Assign IPs on the new subnet:

- `pve2 ‚Üí 10.50.0.20/24`
- `pve3 ‚Üí 10.50.0.30/24`
- `pve4 ‚Üí 10.50.0.40/24`

Ensure this network is **isolated** (no gateway required).

### Verify connectivity
```bash
ping 10.50.0.30
iperf3 -s / -c <peer>
```

---

## 2Ô∏è‚É£ Add the New Public Network (Dual-Network Phase)

> **NOTE:** Back up the file first
```bash
cp /etc/pve/ceph.conf /etc/pve/ceph.conf.bak
```

Edit `/etc/pve/ceph.conf`:

```ini
public_network = 10.50.0.0/24, 172.16.0.0/16
cluster_network = 10.50.0.0/24, 172.16.0.0/16
```

‚ö†Ô∏è **Do NOT remove the old network yet**

Confirm:
- Proxmox UI ‚Üí **Ceph ‚Üí Nodes**
- `ceph config dump`

---

## 3Ô∏è‚É£ Recreate MONs (One by One)

MONs enforce network validation.

For each node:

```bash
pveceph mon destroy <node>
pveceph mon create
ceph -s
```

‚úî Ensure quorum after each step.

---

## 4Ô∏è‚É£ Recreate MGRs (One by One)

- Recreate **standby managers first**
- Leave the **active manager for last**

```bash
pveceph mgr destroy <node>
pveceph mgr create
```

Verify:
```bash
ceph mgr dump
```

### üîß Recovery Tip
If a manager fails to start:
```bash
systemctl reset-failed ceph-mgr@<node>
systemctl start ceph-mgr@<node>
```

---

## 5Ô∏è‚É£ Recreate CephFS Metadata Servers (MDS)

> MDS binds its address **at creation time**

```bash
pveceph mds destroy <node>
pveceph mds create
```

‚úî Verify CephFS health before proceeding.

---

## 6Ô∏è‚É£ Remove the Old Public Network

Edit `/etc/pve/ceph.conf` and remove `172.16.0.0/16`:

```ini
public_network = 10.50.0.0/24
cluster_network = 10.50.0.0/24
```

---

## 7Ô∏è‚É£ Recreate MONs, MGRs, and MDS (Again)

This ensures **all control-plane daemons bind exclusively** to the new network.

Order:
1. MONs (one by one)
2. MGRs (standbys first, active last)
3. MDS (one by one)

---

## 8Ô∏è‚É£ Protect the Cluster Before Touching OSDs

```bash
ceph osd set noout
```

---

## 9Ô∏è‚É£ Restart OSDs (Data Plane Migration)

Restart **one OSD at a time**:

```bash
systemctl restart ceph-osd@<id>
ceph -s
```

Wait for:
```
PGs: active+clean
```

Repeat for all OSDs.

---

## üîü Remove Protection

```bash
ceph osd unset noout
```

---

## üîé Verification (Critical)

### 1Ô∏è‚É£ Verify Ceph daemon addresses

```bash
ceph osd metadata <id> | egrep 'front_addr|back_addr'
```

Expected:
- ‚úÖ `front_addr ‚Üí 10.50.0.x`
- ‚úÖ `back_addr ‚Üí 10.50.0.x`
- ‚ùå No `172.16.x.x`

---

### 2Ô∏è‚É£ Verify traffic is using the Ceph fabric

While Ceph is under load:

```bash
ip -s link show vmbr-ceph
```

RX/TX counters should increase, confirming traffic is **not** using the management network.

---

### 3Ô∏è‚É£ Verify raw network performance (iperf3)

> ‚ö†Ô∏è **Important:** `iperf3` must be installed on **all Ceph nodes** to test the fabric correctly.

```bash
apt install iperf3
```

**Correct testing method:**

- Server on one node:
```bash
iperf3 -s
```

- Client on a *different* node:
```bash
iperf3 -c <peer_ip> -P 4
```

Expected for 2.5 GbE Ceph fabric:
- **~2.1‚Äì2.4 Gbit/s**
- Minimal or zero retransmits
- Stable throughput across multiple streams

---

## üö® Troubleshooting: ‚ÄúOSDs Not Reachable / Wrong Subnet‚Äù

### Symptom
```text
osd.X's public address is not in '172.16.x.x/16' subnet
```

### Cause
Ceph config DB or MON/MGR cache still references the old network.

### Fix (Critical)

#### Restart ALL MONs (mandatory)
```bash
systemctl restart ceph-mon@pve2
systemctl restart ceph-mon@pve3
systemctl restart ceph-mon@pve4
```

#### Restart ALL MGRs (mandatory)
```bash
systemctl restart ceph-mgr@pve2
systemctl restart ceph-mgr@pve3
systemctl restart ceph-mgr@pve4
```

#### (Optional) Clean config DB
```bash
ceph config rm global public_network
ceph config rm global cluster_network
ceph config set global public_network 10.50.0.0/24
ceph config set global cluster_network 10.50.0.0/24
```

Restart OSDs again (one by one).

‚úî This should resolve any ‚ÄúOSDs missing / wrong subnet‚Äù cases.

--- 

## ‚ö†Ô∏è Risks Considered

### Why this change is risky

Changing Ceph cluster networking affects quorum, OSD availability, replication traffic, and client IO. Incorrect sequencing can cause data unavailability or permanent loss.

### Failure modes considered
- MON quorum loss
- OSD flapping
- Client IO stalls
- Backfill storms
- Split-brain conditions

### Assumptions
- Single Ceph cluster
- Dedicated replication network (fabric)
- Change executed during low IO window

---

## ‚úÖ Final State

- Dedicated Ceph fabric (2.5 GbE)
- No Ceph traffic on management NIC
- MON / MGR / MDS / OSD fully migrated
- No data loss
- Stable cluster

## üôè Acknowledgements

This migration approach was heavily informed by the following Proxmox forum discussion, which proved critical in resolving address-binding and daemon recreation issues during the Ceph public network transition:

- **Proxmox Forum ‚Äì ‚ÄúCeph: changing public network‚Äù**  
  https://forum.proxmox.com/threads/ceph-changing-public-network.119116/

In particular, the guidance around:
- Temporarily running **dual public networks**
- **Recreating MON, MGR, and MDS daemons** to force address rebinding
- Avoiding full cluster downtime during network migration

was instrumental in achieving a clean, no‚Äìdata-loss migration.

Many thanks to the contributors in that thread for sharing real-world operational experience.
